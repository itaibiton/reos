---
phase: 40-ai-infrastructure-foundation
plan: 03
type: execute
wave: 3
depends_on: ["40-01", "40-02"]
files_modified:
  - convex/ai/chat.ts
  - convex/ai/agent.ts
autonomous: true

must_haves:
  truths:
    - "AI responses stream token-by-token via Convex"
    - "Typing indicator appears within 200ms of sending message"
    - "User can stop/cancel streaming mid-response"
    - "Profile context injected into every AI call"
    - "Summarization triggered when message count exceeds threshold"
  artifacts:
    - path: "convex/ai/chat.ts"
      provides: "Streaming chat action"
      exports: ["sendMessage", "stopGeneration"]
    - path: "convex/ai/agent.ts"
      provides: "Updated agent with context handler"
      contains: "contextHandler"
  key_links:
    - from: "convex/ai/chat.ts"
      to: "convex/ai/context.ts"
      via: "buildProfileContext import"
      pattern: "buildProfileContext"
    - from: "convex/ai/chat.ts"
      to: "convex/ai/threads.ts"
      via: "thread management imports"
      pattern: "getOrCreateThread"
    - from: "convex/ai/chat.ts"
      to: "convex/ai/summarization.ts"
      via: "summarization imports"
      pattern: "summarizeOldMessages"
---

<objective>
Implement the streaming chat action that ties together agent, context, threads, and summarization into a complete AI interaction flow with stop capability.

Purpose: Deliver the end-to-end AI infrastructure that enables streaming responses with persistent memory and profile context.
Output: Working streaming chat action that satisfies all INFRA requirements.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/40-ai-infrastructure-foundation/40-RESEARCH.md
@.planning/phases/40-ai-infrastructure-foundation/40-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create streaming chat action</name>
  <files>convex/ai/chat.ts</files>
  <action>
**Create convex/ai/chat.ts:**

```typescript
import { action, internalAction } from "../_generated/server";
import { internal, api } from "../_generated/api";
import { v } from "convex/values";
import { investorAssistant } from "./agent";
import { shouldSummarize, THRESHOLDS } from "./summarization";

/**
 * Active generation tracking for stop functionality.
 * Maps threadId to abort controller.
 */
const activeGenerations = new Map<string, AbortController>();

/**
 * Send a message to the AI assistant and stream the response.
 *
 * This action:
 * 1. Gets or creates the user's thread
 * 2. Loads profile context from questionnaire
 * 3. Streams the AI response with delta saving
 * 4. Triggers summarization if needed
 *
 * Per CONTEXT.md:
 * - Fresh start visual: User sees new session, but AI has memory
 * - Profile data always in context (never summarized)
 * - Typing indicator supported via streaming infrastructure
 */
export const sendMessage = action({
  args: {
    message: v.string(),
  },
  handler: async (ctx, { message }) => {
    // Get authenticated user
    const identity = await ctx.auth.getUserIdentity();
    if (!identity) {
      throw new Error("Not authenticated");
    }

    // Get user from database
    const user = await ctx.runQuery(api.users.getByClerkId, {
      clerkId: identity.subject,
    });
    if (!user) {
      throw new Error("User not found");
    }

    // Get or create thread
    const threadResult = await ctx.runMutation(api.ai.threads.getOrCreateThread, {});
    const { threadId, agentThreadId, isNew } = threadResult;

    // Load profile context
    const profileContext = await ctx.runQuery(internal.ai.context.buildProfileContext, {
      userId: user._id,
    });

    // Load thread summary if exists (for long conversations)
    const thread = await ctx.runQuery(api.ai.threads.getThreadForUser, {});
    const summary = thread?.summary;

    // Build system prompt with profile context
    let systemPrompt = investorAssistant.instructions || "";
    if (profileContext) {
      systemPrompt += `\n\n${profileContext}`;
    }
    if (summary) {
      systemPrompt += `\n\n## Previous Conversation Summary\n\n${summary}\n\n---\nNote: I'm focusing on our recent discussion. The above summarizes our earlier conversation.`;
    }

    // Create abort controller for this generation
    const abortController = new AbortController();
    activeGenerations.set(threadId, abortController);

    try {
      // Create or continue agent thread
      let agentThread;
      if (agentThreadId) {
        // Continue existing thread
        agentThread = await investorAssistant.continueThread(ctx, {
          threadId: agentThreadId,
        });
      } else {
        // Create new thread
        agentThread = await investorAssistant.createThread(ctx, {
          systemPrompt,
        });

        // Link the agent thread to our application thread
        await ctx.runMutation(internal.ai.threads.linkAgentThread, {
          threadId,
          agentThreadId: agentThread.threadId,
        });
      }

      // Stream the response with delta saving
      // This persists streaming state to database for real-time UI updates
      const result = await agentThread.thread.streamText(
        { prompt: message },
        {
          saveStreamDeltas: {
            chunking: "word", // Buffer by word for smoother display
            throttleMs: 50, // Update every 50ms for responsive feel
          },
          abortSignal: abortController.signal,
        }
      );

      // Clean up abort controller
      activeGenerations.delete(threadId);

      // Check if summarization needed
      // Note: We need to get message count from agent thread
      // This is a simplified check - full implementation would query agent messages
      const messageCount = await getApproximateMessageCount(ctx, agentThread.threadId);
      if (shouldSummarize(messageCount)) {
        // Trigger async summarization (don't await - let it run in background)
        ctx.runAction(internal.ai.summarization.summarizeOldMessages, {
          threadId,
          messages: await getMessagesForSummarization(ctx, agentThread.threadId),
        }).catch((err) => console.error("Summarization failed:", err));
      }

      return {
        success: true,
        messageId: result.messageId,
        threadId,
        agentThreadId: agentThread.threadId,
      };
    } catch (error) {
      // Clean up on error
      activeGenerations.delete(threadId);

      // Check if this was a user-initiated stop
      if (error instanceof Error && error.name === "AbortError") {
        return {
          success: true,
          stopped: true,
          threadId,
        };
      }

      throw error;
    }
  },
});

/**
 * Stop an active AI generation.
 *
 * Per CONTEXT.md: "Visible stop button during streaming - user can interrupt immediately"
 */
export const stopGeneration = action({
  args: {},
  handler: async (ctx) => {
    const identity = await ctx.auth.getUserIdentity();
    if (!identity) {
      throw new Error("Not authenticated");
    }

    // Get the user's thread
    const thread = await ctx.runQuery(api.ai.threads.getThreadForUser, {});
    if (!thread) {
      return { stopped: false, reason: "No active thread" };
    }

    const threadId = thread._id;
    const controller = activeGenerations.get(threadId);

    if (controller) {
      controller.abort();
      activeGenerations.delete(threadId);
      return { stopped: true };
    }

    return { stopped: false, reason: "No active generation" };
  },
});

/**
 * Get streaming status for UI typing indicator.
 * The @convex-dev/agent handles this internally via subscriptions,
 * but we expose it for UI convenience.
 */
export const getStreamingStatus = action({
  args: {},
  handler: async (ctx) => {
    const identity = await ctx.auth.getUserIdentity();
    if (!identity) {
      return { isStreaming: false };
    }

    const thread = await ctx.runQuery(api.ai.threads.getThreadForUser, {});
    if (!thread) {
      return { isStreaming: false };
    }

    const isStreaming = activeGenerations.has(thread._id);
    return { isStreaming };
  },
});

/**
 * Helper to get approximate message count from agent thread.
 * Used for summarization threshold check.
 */
async function getApproximateMessageCount(
  ctx: any,
  agentThreadId: string
): Promise<number> {
  // The @convex-dev/agent component tracks messages internally
  // For now, return a conservative estimate
  // Full implementation would query the agent's message storage
  try {
    const messages = await investorAssistant.listMessages(ctx, {
      threadId: agentThreadId,
      limit: THRESHOLDS.SUMMARIZE_THRESHOLD + 5,
    });
    return messages.length;
  } catch {
    return 0;
  }
}

/**
 * Helper to get messages for summarization.
 */
async function getMessagesForSummarization(
  ctx: any,
  agentThreadId: string
): Promise<Array<{ role: "user" | "assistant"; content: string }>> {
  try {
    const messages = await investorAssistant.listMessages(ctx, {
      threadId: agentThreadId,
    });
    return messages.map((m: any) => ({
      role: m.role as "user" | "assistant",
      content: m.content,
    }));
  } catch {
    return [];
  }
}
```

This action:
- Creates/continues agent threads
- Injects profile context into system prompt
- Injects conversation summary for long threads
- Streams responses with saveStreamDeltas
- Supports abort/stop functionality
- Triggers background summarization when threshold exceeded
  </action>
  <verify>
```bash
npx convex dev --once
```
Should compile without errors.
  </verify>
  <done>convex/ai/chat.ts exists with sendMessage (streaming), stopGeneration, and getStreamingStatus actions</done>
</task>

<task type="auto">
  <name>Task 2: Update agent with context handler</name>
  <files>convex/ai/agent.ts</files>
  <action>
**Update convex/ai/agent.ts** to add the context options that support our memory strategy:

```typescript
import { Agent } from "@convex-dev/agent";
import { anthropic } from "@ai-sdk/anthropic";
import { components } from "../_generated/api";

/**
 * Investor Assistant Agent
 *
 * This is the core AI agent for the investor experience.
 * It has access to investor profiles and can answer questions about:
 * - Investment preferences and goals
 * - Property recommendations (Phase 42)
 * - Provider suggestions (Phase 43)
 *
 * Memory persists across sessions via Convex.
 *
 * Context Strategy:
 * - Profile data: Always in system prompt (never summarized)
 * - Recent messages: Last 10 kept verbatim
 * - Older messages: Summarized when > 15 total
 * - Summary: Prepended as system context
 */
export const investorAssistant = new Agent(components.agent, {
  name: "Investor Assistant",
  chat: anthropic.chat("claude-sonnet-4-20250514"),
  instructions: `You are a helpful real estate investment assistant for REOS, a platform connecting US investors with Israeli properties.

Your role:
- Help investors understand their investment options
- Answer questions about the investor's profile and preferences
- Provide guidance on the real estate investment process in Israel
- Be accurate and helpful - never make up information about properties or providers

Guidelines:
- Be conversational but professional
- Reference the investor's profile when relevant, but don't repeat it unnecessarily
- If asked about something not in your context, acknowledge the limitation
- For property-specific questions, note that property search and recommendations will be enhanced in future updates
- When you sense the conversation has been long, you may acknowledge "I'm focusing on our recent discussion" if context seems compressed

Memory behavior:
- You have persistent memory of past conversations with this investor
- Acknowledge past context only when directly relevant
- If profile data seems outdated, suggest the investor update their preferences
- When conversation contradicts profile, ask to clarify which is current

Current capabilities:
- Understanding investor profiles (from questionnaire data)
- Answering general investment questions
- Providing guidance on the US-Israel investment process`,

  // Context management options
  contextOptions: {
    recentMessages: 10, // Keep last 10 messages verbatim (matches KEEP_RECENT threshold)
  },

  tools: {}, // Tools will be added in Phase 42 (properties) and Phase 43 (providers)
  maxSteps: 5,
});

/**
 * Export for use in other modules
 */
export type InvestorAssistant = typeof investorAssistant;
```

The updated agent includes:
- Expanded instructions about memory behavior (per CONTEXT.md)
- contextOptions.recentMessages set to 10 (matching our KEEP_RECENT threshold)
- Type export for use in chat.ts
  </action>
  <verify>
```bash
npx convex dev --once
```
Should compile without errors.
  </verify>
  <done>convex/ai/agent.ts updated with contextOptions, memory behavior instructions, and type export</done>
</task>

<task type="auto">
  <name>Task 3: Add users query for chat action</name>
  <files>convex/users.ts</files>
  <action>
The chat.ts action needs a query to get user by Clerk ID. Add this query to convex/users.ts.

**Add the following export to convex/users.ts:**

First, verify the imports are present at the top of the file. The file should already have:
```typescript
import { query, mutation } from "./_generated/server";
import { v } from "convex/values";
```

Then add this query (it may not exist yet):

```typescript
/**
 * Get user by Clerk ID (for internal use by AI actions)
 */
export const getByClerkId = query({
  args: {
    clerkId: v.string(),
  },
  handler: async (ctx, { clerkId }) => {
    return await ctx.db
      .query("users")
      .withIndex("by_clerk_id", (q) => q.eq("clerkId", clerkId))
      .unique();
  },
});
```

This query allows the chat action to look up the user document from the Clerk identity.

**Note:** If this query already exists, no action is needed.
  </action>
  <verify>
```bash
npx convex dev --once
```
Convex should compile without errors, confirming the query is available.
  </verify>
  <done>convex/users.ts includes getByClerkId query that chat.ts can use</done>
</task>

</tasks>

<verification>
1. `npx convex dev --once` completes without errors
2. convex/ai/chat.ts exports sendMessage, stopGeneration, getStreamingStatus
3. convex/ai/agent.ts has contextOptions and updated instructions
4. convex/users.ts has getByClerkId query
5. All imports between ai/ files resolve correctly
</verification>

<success_criteria>
- Streaming chat action wires together all infrastructure pieces
- Stop button functionality implemented via abort controller
- Profile context injected from questionnaire data
- Conversation summary injected for long threads
- Summarization triggered in background when threshold exceeded
- Agent has proper context options and memory behavior instructions
- All Convex types pass validation
</success_criteria>

<output>
After completion, create `.planning/phases/40-ai-infrastructure-foundation/40-03-SUMMARY.md`
</output>
